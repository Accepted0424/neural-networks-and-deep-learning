{"cells":[{"cell_type":"markdown","id":"445e4f40","metadata":{"id":"445e4f40"},"source":["## 用于手写数字识别的简单神经网络（基于MINIST数据集）"]},{"cell_type":"markdown","id":"9140514b","metadata":{"id":"9140514b"},"source":["### 环境准备"]},{"cell_type":"code","execution_count":29,"id":"98d3a8b6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7811,"status":"ok","timestamp":1745751306837,"user":{"displayName":"罗浩宇","userId":"13944575000275177578"},"user_tz":-480},"id":"98d3a8b6","outputId":"ec5fb1c6-e968-417e-e82c-5aec9755f26e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n"]}],"source":["pip install numpy"]},{"cell_type":"code","execution_count":30,"id":"bb357545-7384-4a32-a2cc-e9b282dfdcfb","metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1745751306857,"user":{"displayName":"罗浩宇","userId":"13944575000275177578"},"user_tz":-480},"id":"bb357545-7384-4a32-a2cc-e9b282dfdcfb"},"outputs":[],"source":["#### Libraries\n","# Standard library\n","import random\n","import time\n","import pickle\n","import gzip\n","\n","# Third-party libraries\n","import numpy as np"]},{"cell_type":"markdown","id":"5bf62194","metadata":{"id":"5bf62194"},"source":["### 数据定义\n","- sizes是一个列表（List），通常用于表示神经网络中各层的神经元数量\n","  - 784：输入层的神经元数量。对于 MNIST 数据集，每个图像是 28x28 像素，展开成一个一维的 784 元素向量，因此输入层有 784 个神经元\n","  - 30：隐藏层的神经元数量。这里是一个单独的隐藏层，包含 30 个神经元，通常可以根据具体的任务来调整这个数量\n","  - 10：输出层的神经元数量。对于 MNIST 数据集，输出是一个 10 类分类任务（数字 0 到 9），所以输出层有 10 个神经元，每个神经元表示一个类别的概率\n","- num_layers是神经网络中层的数量，数值为3，表示这个神经网络有三层，即输入层、隐藏层和输出层\n","- biases 随机初始化隐藏层和输出层的偏置\n","  - 第一个元素是一个(30, 1)的数组(30行1列的二维数组)，表示隐藏层的 30 个神经元的偏置\n","  - 第二个元素是一个(10, 1)的数组，表示输出层的10个神经元的偏置\n","- weights 随机初始化权重矩阵\n","  - `zip(sizes[:-1], sizes[1:])` 会得到 [(784, 30), (30, 10)]\n","  - 输入层到隐藏层的权重矩阵形状是 (30, 784)，隐藏层到输出层的权重矩阵形状是 (10, 30)"]},{"cell_type":"code","execution_count":31,"id":"873d4f8d","metadata":{"executionInfo":{"elapsed":34,"status":"ok","timestamp":1745751306892,"user":{"displayName":"罗浩宇","userId":"13944575000275177578"},"user_tz":-480},"id":"873d4f8d"},"outputs":[],"source":["sizes = [784, 30, 10]\n","num_layers = len(sizes)\n","biases = [np.random.randn(y, 1) for y in sizes[1:]]\n","weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]"]},{"cell_type":"markdown","id":"501191a3-ae53-47b1-b093-a548a0b06344","metadata":{"id":"501191a3-ae53-47b1-b093-a548a0b06344"},"source":["### 加载数据集\n","- 加载一个压缩的 mnist.pkl.gz 文件，其中包含了MINIST数据集，'rb'表示以二进制模式打开该文件\n","- MNIST数据集在保存时使用了'latin1'编码，需指定解包器使用该编码\n","- u.load()解包数据。这些数据将用于训练、验证和测试神经网络模型。"]},{"cell_type":"code","execution_count":32,"id":"2e9870ed","metadata":{"executionInfo":{"elapsed":2140,"status":"ok","timestamp":1745751309033,"user":{"displayName":"罗浩宇","userId":"13944575000275177578"},"user_tz":-480},"id":"2e9870ed"},"outputs":[],"source":["f = gzip.open('mnist.pkl.gz', 'rb')\n","u = pickle._Unpickler(f)\n","u.encoding = 'latin1'\n","tr_d, va_d, te_d = u.load()\n","f.close()"]},{"cell_type":"markdown","id":"7b16b9c5","metadata":{"id":"7b16b9c5"},"source":["### 格式化数据\n","- tr_d[0]是一个列表，每个元素是一个784维的向量(表示一个28像素*28像素的图像)，我们将其转换为784行1列的列向量\n","- tr_d[1]是一个列表，每个元素是一个数字(0-9)，称为标签，表示图像对应的数字，我们将其转换为一个 10 维的单位向量\n","- 将training_inputs和training_results一一对应拼接起来形成格式化的训练数据集traning_data\n","- 对验证集和测试机做相同的操作，不过数据的标签不需要向量化"]},{"cell_type":"code","execution_count":33,"id":"6d31cdbf","metadata":{"executionInfo":{"elapsed":235,"status":"ok","timestamp":1745751309267,"user":{"displayName":"罗浩宇","userId":"13944575000275177578"},"user_tz":-480},"id":"6d31cdbf"},"outputs":[],"source":["def vectorized_result(j):\n","    e = np.zeros((10, 1))\n","    e[j] = 1.0\n","    return e\n","\n","training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n","training_results = [vectorized_result(y) for y in tr_d[1]]\n","training_data = list(zip(training_inputs, training_results))\n","validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n","validation_data = list(zip(validation_inputs, va_d[1]))\n","test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n","test_data = list(zip(test_inputs, te_d[1]))"]},{"cell_type":"markdown","id":"3d522b9e","metadata":{"id":"3d522b9e"},"source":["### 训练数据定义\n","你可以尝试修改这些参数，体会其对训练结果的影响"]},{"cell_type":"code","execution_count":34,"id":"40fb9e35","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1745751309273,"user":{"displayName":"罗浩宇","userId":"13944575000275177578"},"user_tz":-480},"id":"40fb9e35"},"outputs":[],"source":["n_traning = len(training_data) # 训练集的图片数量\n","n_test = len(test_data) # 测试集的图片数量\n","epochs = 30 # 训练的轮数\n","mini_batch_size = 10 # 每个小批量的大小\n","eta = 1.0 # 学习率"]},{"cell_type":"markdown","id":"9107e930","metadata":{"id":"9107e930"},"source":["### 训练模型函数 (随机梯度下降)"]},{"cell_type":"markdown","id":"7e98e0d6","metadata":{"id":"7e98e0d6"},"source":["#### 工具函数\n","**sigmod函数**\n","\n","在神经网络里，Sigmoid 常用作神经元的激活函数，把任意输入的实数z压缩到 (0,1) 之间\n","$$\n","\\sigma(z) = \\frac{1}{1 + e^{-z}}\n","$$\n","\n","**sigmod函数的导数**\n","\n","Sigmoid 函数在任何点都是可导的，而且导数有漂亮的形式（推导简单，方便反向传播训练）\n","$$\n","\\sigma'(z) = \\sigma(z) \\times (1 - \\sigma(z))\n","$$"]},{"cell_type":"code","execution_count":35,"id":"ad55b8d9","metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1745751309289,"user":{"displayName":"罗浩宇","userId":"13944575000275177578"},"user_tz":-480},"id":"ad55b8d9"},"outputs":[],"source":["def cost_derivative(output_activations, y):\n","        \"\"\"Return the vector of partial derivatives \\partial C_x /\n","        \\partial a for the output activations.\"\"\"\n","        return (output_activations-y)\n","\n","def sigmoid(z):\n","    \"\"\"The sigmoid function.\"\"\"\n","    return 1.0/(1.0+np.exp(-z))\n","\n","def sigmoid_prime(z):\n","    \"\"\"Derivative of the sigmoid function.\"\"\"\n","    return sigmoid(z)*(1-sigmoid(z))"]},{"cell_type":"markdown","id":"2ea8e37d","metadata":{"id":"2ea8e37d"},"source":["#### 反向传播函数\n","**第一层（输入层到第一隐藏层）特例**：\n","\n","$$\n","z^{(1)} = W^{(1)} \\cdot x + b^{(1)}\n","$$\n","\n","其中 \\( x \\) 是输入向量。\n","\n","**每一层的输出（以第 \\( l \\) 层为例）**：\n","\n","$$\n","z^{(l)} = W^{(l)} \\cdot \\sigma\\left(z^{(l-1)}\\right) + b^{(l)}\n","$$\n","\n","**损失函数**:\n","$$\n","C = \\frac{1}{2} \\| a - y \\|^2 \\\\\n","C' = a - y\n","$$\n","\n","**计算输出层的误差**：\n","$$\n","\\delta^{(L)} = \\nabla_a C \\odot \\sigma'\\left(z^{(L)}\\right) \\\\\n","= (\\sigma\\left(z^{(L)}\\right) - y) \\times \\sigma'\\left(z^{(L)}\\right)\n","$$\n","- $\\odot$ 在数学里表示的是 Hadamard积，两个同形状的向量或矩阵，对应位置的元素分别相乘。这里输出和标准输出均为0维向量（单个数字），可以相乘。\n","\n","**输出层的梯度**:\n","\n","梯度告诉我们如何调整每个偏置和权重，使得损失函数减少。\n","- 偏置的梯度即为误差\n","- 权重的梯度为误差点乘前一层激活值的转置。可以理解为第 L 层的权重梯度是当前层的误差项与前一层激活值的外积\n","$$\n","\\nabla_b^{(L)} = \\delta^{(L)}\n","$$\n","\n","$$\n","\\nabla_w^{(L)} = \\delta^{(L)} \\cdot \\left(a^{(L-1)}\\right)^T\n","$$\n","\n","**向后传播误差**:\n","$$\n","\\delta^{(l)} = \\left(W^{(l+1)}\\right)^T \\delta^{(l+1)} \\odot \\sigma'\\left(z^{(l)}\\right)\n","$$\n","\n","$$\n","\\nabla_b^{(l)} = \\delta^{(l)}\n","$$\n","\n","$$\n","\\nabla_w^{(l)} = \\delta^{(l)} \\cdot \\left(a^{(l-1)}\\right)^T\n","$$\n","\n","\n"]},{"cell_type":"code","execution_count":36,"id":"359a3dca","metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1745751309302,"user":{"displayName":"罗浩宇","userId":"13944575000275177578"},"user_tz":-480},"id":"359a3dca"},"outputs":[],"source":["# x是每张手写图的向量形式输入，y是标准答案\n","def backprop(x, y):\n","   # 初始化偏置和权重的梯度\n","    nabla_b = [np.zeros(b.shape) for b in biases]\n","    nabla_w = [np.zeros(w.shape) for w in weights]\n","\n","    # 向前传播\n","    zs = [] # 存储每一层的输出向量\n","    activation = x\n","    activations = [x] # 存储每一层输出向量的激活值（归一化）\n","    for b, w in zip(biases, weights):\n","        z = np.dot(w, activation)+b # 计算每一层的输出向量\n","        zs.append(z)\n","        activation = sigmoid(z) # 计算每一层输出向量的激活值\n","        activations.append(activation)\n","\n","    # 反向传播\n","    delta = cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1]) # 计算输出层与标准答案的误差\n","    nabla_b[-1] = delta\n","    nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n","\n","    # 反向传播误差到每一层\n","    for l in range(2, num_layers):\n","        delta = np.dot(weights[-l+1].transpose(), delta) * sigmoid_prime(zs[-l])\n","        nabla_b[-l] = delta\n","        nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n","    return (nabla_b, nabla_w)"]},{"cell_type":"markdown","id":"78571769","metadata":{"id":"78571769"},"source":["### 模型评估函数\n","所谓评估，就是拿训练好的神经网络去测试训练集"]},{"cell_type":"code","execution_count":37,"id":"8adc627a","metadata":{"executionInfo":{"elapsed":47,"status":"ok","timestamp":1745751309351,"user":{"displayName":"罗浩宇","userId":"13944575000275177578"},"user_tz":-480},"id":"8adc627a"},"outputs":[],"source":["def feedforward(a):\n","    for b, w in zip(biases, weights):\n","        a = sigmoid(np.dot(w, a)+b)\n","    return a\n","\n","def evaluate(test_data):\n","    test_results = [(np.argmax(feedforward(x)), y)\n","                    for (x, y) in test_data]\n","    return sum(int(x == y) for (x, y) in test_results)"]},{"cell_type":"markdown","id":"76ab63ed","metadata":{"id":"76ab63ed"},"source":["### 训练并评估\n","**训练数据预处理**\n","- 每一轮训练前打乱训练数据的顺序，防止学习过程中模型对数据顺序产生依赖\n","- 将数据分成一小批一小批的数据（mini-batches），提高效率，也能让模型收敛得更好"]},{"cell_type":"code","execution_count":null,"id":"6c6b9fca","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"6c6b9fca"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0: 7875 / 10000, took 17.01 seconds\n","Epoch 1: 8206 / 10000, took 7.07 seconds\n","Epoch 2: 8311 / 10000, took 6.26 seconds\n","Epoch 3: 9144 / 10000, took 7.88 seconds\n","Epoch 4: 9255 / 10000, took 5.86 seconds\n","Epoch 5: 9294 / 10000, took 7.00 seconds\n","Epoch 6: 9297 / 10000, took 5.92 seconds\n","Epoch 7: 9319 / 10000, took 6.90 seconds\n","Epoch 8: 9358 / 10000, took 6.80 seconds\n","Epoch 9: 9353 / 10000, took 6.55 seconds\n","Epoch 10: 9378 / 10000, took 6.60 seconds\n","Epoch 11: 9402 / 10000, took 6.35 seconds\n","Epoch 12: 9399 / 10000, took 6.96 seconds\n","Epoch 13: 9401 / 10000, took 6.44 seconds\n","Epoch 14: 9438 / 10000, took 7.19 seconds\n","Epoch 15: 9419 / 10000, took 5.94 seconds\n","Epoch 16: 9413 / 10000, took 7.26 seconds\n","Epoch 17: 9441 / 10000, took 5.99 seconds\n","Epoch 18: 9447 / 10000, took 7.87 seconds\n","Epoch 19: 9444 / 10000, took 5.90 seconds\n","Epoch 20: 9461 / 10000, took 6.83 seconds\n","Epoch 21: 9456 / 10000, took 6.16 seconds\n","Epoch 22: 9458 / 10000, took 6.64 seconds\n","Epoch 23: 9475 / 10000, took 7.55 seconds\n","Epoch 24: 9462 / 10000, took 5.89 seconds\n","Epoch 25: 9461 / 10000, took 6.84 seconds\n","Epoch 26: 9470 / 10000, took 5.91 seconds\n","Epoch 27: 9457 / 10000, took 6.89 seconds\n","Epoch 28: 9476 / 10000, took 5.89 seconds\n","Epoch 29: 9471 / 10000, took 7.06 seconds\n"]}],"source":["for i in range(epochs):\n","    time1 = time.time()\n","    random.shuffle(training_data) # 打乱训练集的顺序\n","    mini_batches = [training_data[k:k + 10] for k in range(0, n_traning, 10)] # 每个小批量包含10张图片\n","    mini_batches = [\n","                training_data[k:k+mini_batch_size]\n","                for k in range(0, n_traning, mini_batch_size)]\n","    for mini_batch in mini_batches:\n","        nabla_b = [np.zeros(b.shape) for b in biases]\n","        nabla_w = [np.zeros(w.shape) for w in weights]\n","        for x, y in mini_batch:\n","            delta_nabla_b, delta_nabla_w = backprop(x, y)\n","            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n","            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n","        weights = [w-(eta/len(mini_batch))*nw\n","                    for w, nw in zip(weights, nabla_w)]\n","        biases = [b-(eta/len(mini_batch))*nb\n","                   for b, nb in zip(biases, nabla_b)]\n","    time2 = time.time()\n","    if test_data:\n","        print(\"Epoch {0}: {1} / {2}, took {3:.2f} seconds\".format(i, evaluate(test_data), n_test, time2-time1))\n","    else:\n","        print(\"Epoch {0} complete in {1:.2f} seconds\".format(i, time2-time1))"]}],"metadata":{"colab":{"name":"","toc_visible":true,"version":""},"kernelspec":{"display_name":"deepLearning","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.16"}},"nbformat":4,"nbformat_minor":5}